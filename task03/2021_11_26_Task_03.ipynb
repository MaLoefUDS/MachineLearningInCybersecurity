{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKo3S_fOsMim"
   },
   "source": [
    "# ML in Cybersecurity: Task 3\n",
    "\n",
    "## Team\n",
    "  * **Team name**:  :flushed:\n",
    "  * **Members**:  Tim Schneider (s8tiscne@stud.uni-saarland.de), Qiankun Zheng (qizh00001@stud.uni-saarland.de), Maximilian Löffler (s8maloef@stud.uni-saarland.de)\n",
    "\n",
    "\n",
    "## Logistics\n",
    "  * **Due date**: 9th December 2021, 23:59:59\n",
    "  * Email the completed notebook to: `mlcysec_ws2022_staff@lists.cispa.saarland`\n",
    "  * Complete this in **teams of 3**\n",
    "  * Feel free to use the forum to discuss.\n",
    "  \n",
    "## Timeline\n",
    "  * 26-Nov-2021: hand-out\n",
    "  * **09-Dec-2021**: Email completed notebook\n",
    "  \n",
    "  \n",
    "## About this Project\n",
    "In this project, you will explore an application of ML to a popular task in cybersecurity: malware classification.\n",
    "You will be presented with precomputed behaviour analysis reports of thousands of program binaries, many of which are malwares.\n",
    "Your goal is to train a malware detector using this behavioural reports.\n",
    "\n",
    "\n",
    "## A Note on Grading\n",
    "The grading for this project will depend on:\n",
    " 1. Vectorizing Inputs\n",
    "   * Obtaining a reasonable vectorized representations of the input data (a file containing a sequence of system calls)\n",
    "   * Understanding the influence these representations have on your model\n",
    " 1. Classification Model  \n",
    "   * Following a clear ML pipeline\n",
    "   * Obtaining reasonable performances (>60\\%) on held-out test set\n",
    "   * Choice of evaluation metric\n",
    "   * Visualizing loss/accuracy curves\n",
    " 1. Analysis\n",
    "   * Which methods (input representations/ML models) work better than the rest and why?\n",
    "   * Which hyper-parameters and design-choices were important in each of your methods?\n",
    "   * Quantifying influence of these hyper-parameters on loss and/or validation accuracies\n",
    "   * Trade-offs between methods, hyper-parameters, design-choices\n",
    "   * Anything else you find interesting (this part is open-ended)\n",
    "\n",
    "\n",
    "## Grading Details\n",
    " * 40 points: Vectorizing input data (each input = behaviour analysis file in our case)\n",
    " * 40 points: Training a classification model\n",
    " * 15 points: Analysis/Discussion\n",
    " * 5 points: Clean code\n",
    " \n",
    "## Filling-in the Notebook\n",
    "You'll be submitting this very notebook that is filled-in with your code and analysis. Make sure you submit one that has been previously executed in-order. (So that results/graphs are already visible upon opening it). \n",
    "\n",
    "The notebook you submit **should compile** (or should be self-contained and sufficiently commented). Check tutorial 1 on how to set up the Python3 environment.\n",
    "\n",
    "\n",
    "**The notebook is your project report. So, to make the report readable, omit code for techniques/models/things that did not work. You can use the final summary to provide a report about these.**\n",
    "\n",
    "It is extremely important that you **do not** re-order the existing sections. Apart from that, the code blocks that you need to fill-in are given by:\n",
    "```\n",
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "```\n",
    "Feel free to break this into multiple-cells. It's even better if you interleave explanations and code-blocks so that the entire notebook forms a readable \"story\".\n",
    "\n",
    "\n",
    "## Code of Honor\n",
    "We encourage discussing ideas and concepts with other students to help you learn and better understand the course content. However, the work you submit and present **must be original** and demonstrate your effort in solving the presented problems. **We will not tolerate** blatantly using existing solutions (such as from the internet), improper collaboration (e.g., sharing code or experimental data between groups) and plagiarism. If the honor code is not met, no points will be awarded.\n",
    "\n",
    " \n",
    " ## Versions\n",
    "  * v1.1: Updated deadline\n",
    "  * v1.0: Initial notebook\n",
    "  \n",
    "  ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uyCiLbXbsMiq"
   },
   "outputs": [],
   "source": [
    "import time \n",
    " \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import json \n",
    "import time \n",
    "import pickle \n",
    "import sys \n",
    "import csv \n",
    "import os \n",
    "import os.path as osp \n",
    "import shutil \n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import display, HTML\n",
    " \n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots \n",
    "plt.rcParams['image.interpolation'] = 'nearest' \n",
    "plt.rcParams['image.cmap'] = 'gray' \n",
    " \n",
    "# for auto-reloading external modules \n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7dHogdxRsMis"
   },
   "outputs": [],
   "source": [
    "# Some suggestions of our libraries that might be helpful for this project\n",
    "from collections import Counter          # an even easier way to count\n",
    "from multiprocessing import Pool         # for multiprocessing\n",
    "from tqdm import tqdm                    # fancy progress bars\n",
    "\n",
    "# Load other libraries here.\n",
    "# Keep it minimal! We should be easily able to reproduce your code.\n",
    "\n",
    "# We preload pytorch as an example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "\n",
    "def loading(part, whole):\n",
    "    assert(isinstance(part, int))\n",
    "    assert(isinstance(whole, int))\n",
    "    partial = (part + 1) * 100 / whole\n",
    "    sys.stdout.write('\\r{}%\\r'.format(int(partial * 100) / 100))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "malware_map = {\n",
    "    'NothingFound': 0,\n",
    "    'Basun': 1,\n",
    "    'Patched': 2,\n",
    "    'Texel': 3,\n",
    "    'Swizzor': 4,\n",
    "    'Virut': 5,\n",
    "    'VB': 6,\n",
    "    'AutoIt': 7,\n",
    "    'Agent': 8,\n",
    "    'Allaple': 9\n",
    "}\n",
    "    \n",
    "def int_to_class(i):    \n",
    "    for key in malware_map.keys():\n",
    "        if malware_map[key] == i:\n",
    "            return key\n",
    "    \n",
    "def class_to_int(c):\n",
    "    return malware_map[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJWkh3GUsMit"
   },
   "source": [
    "# Setup\n",
    "\n",
    "  * Download the datasets: [train](https://nextcloud.mpi-klsb.mpg.de/index.php/s/pJrRGzm2So2PMZm) (128M) and [test](https://nextcloud.mpi-klsb.mpg.de/index.php/s/zN3yeWzQB3i5WqE) (92M)\n",
    "  * Unpack them under `./data/train` and `./data/test`\n",
    "  * Hint: you can execute shell scripts from notebooks using the `!` prefix, e.g., `! wget <url>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wlhs4w44sMit"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train examples (Should be 13682) :    13682\n",
      "# test  examples (Should be 10000) :    10000\n"
     ]
    }
   ],
   "source": [
    "# Check that you are prepared with the data\n",
    "! printf '# train examples (Should be 13682) : '; ls data/train | wc -l\n",
    "! printf '# test  examples (Should be 10000) : '; ls data/test | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zh1-JHNIsMiv"
   },
   "source": [
    "Now that you're set, let's briefly look at the data you have been handed.\n",
    "Each file encodes the behavior report of a program (potentially a malware), using an encoding scheme called \"The Malware Instruction Set\" (MIST for short).\n",
    "At this point, we highly recommend you briefly read-up Sec. 2 of the [MIST](http://www.mlsec.org/malheur/docs/mist-tr.pdf) documentation.\n",
    "\n",
    "You will find each file named as `filename.<malwarename>`:\n",
    "```\n",
    "» ls data/train | head\n",
    "00005ecc06ae3e489042e979717bb1455f17ac9d.NothingFound\n",
    "0008e3d188483aeae0de62d8d3a1479bd63ed8c9.Basun\n",
    "000d2eea77ee037b7ef99586eb2f1433991baca9.Patched\n",
    "000d996fa8f3c83c1c5568687bb3883a543ec874.Basun\n",
    "0010f78d3ffee61101068a0722e09a98959a5f2c.Basun\n",
    "0013cd0a8febd88bfc4333e20486bd1a9816fcbf.Basun\n",
    "0014aca72eb88a7f20fce5a4e000c1f7fff4958a.Texel\n",
    "001ffc75f24a0ae63a7033a01b8152ba371f6154.Texel\n",
    "0022d6ba67d556b931e3ab26abcd7490393703c4.Basun\n",
    "0028c307a125cf0fdc97d7a1ffce118c6e560a70.Swizzor\n",
    "...\n",
    "```\n",
    "and within each file, you will see a sequence of individual systems calls monitored duing the run-time of the binary - a malware named 'Basun' in the case:\n",
    "```\n",
    "» head data/train/000d996fa8f3c83c1c5568687bb3883a543ec874.Basun\n",
    "# process 000006c8 0000066a 022c82f4 00000000 thread 0001 #\n",
    "02 01 | 000006c8 0000066a 00015000\n",
    "02 02 | 00006b2c 047c8042 000b9000\n",
    "02 02 | 00006b2c 047c8042 00108000\n",
    "02 02 | 00006b2c 047c8042 00153000\n",
    "02 02 | 00006b2c 047c8042 00091000\n",
    "02 02 | 00006b2c 047c8042 00049000\n",
    "02 02 | 00006b2c 047c8042 000aa000\n",
    "02 02 | 00006b2c 047c8042 00092000\n",
    "02 02 | 00006b2c 047c8042 00011000\n",
    "...\n",
    "```\n",
    "(**Note**: Please ignore the first line that begins with `# process ...`.)\n",
    "\n",
    "Your task in this project is to train a malware detector, which given the sequence of system calls (in the MIST-formatted file like above), predicts one of 10 classes: `{ Agent, Allaple, AutoIt, Basun, NothingFound, Patched, Swizzor, Texel, VB, Virut }`, where `NothingFound` roughly represents no malware is present.\n",
    "In terms of machine learning terminology, your malware detector $F: X \\rightarrow Y$ should learn a mapping from the MIST-encoded behaviour report (the input $x \\in X$) to the malware class $y \\in Y$.\n",
    "\n",
    "Consequently, you will primarily tackle two challenges in this project:\n",
    "  1. \"Vectorizing\" the input data i.e., representing each input (file) as a tensor\n",
    "  1. Training an ML model\n",
    "  \n",
    "\n",
    "### Some tips:\n",
    "  * Begin with an extremely simple representation/ML model and get above chance-level classification performance\n",
    "  * Choose your evaluation metric wisely\n",
    "  * Save intermediate computations (e.g., a token to index mapping). This will avoid you parsing the entire dataset for every experiment\n",
    "  * Try using `multiprocessing.Pool` to parallelize your `for` loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8xloR8dsMiv"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXnyDAdbsMiw"
   },
   "source": [
    "# 1. Vectorize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ia-xFIQtsMiw"
   },
   "source": [
    "## 1.a. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZScY-C2IsMiw"
   },
   "outputs": [],
   "source": [
    "def load_content(filepath):\n",
    "    '''Given a filepath, returns (content, classname), where content = [list of lines in file]'''\n",
    "    with open(filepath) as file:\n",
    "        lines = list(map(lambda line: line[:-2], file.readlines()))\n",
    "    return lines, filepath.split(\".\")[2]\n",
    "\n",
    "\n",
    "def load_data(data_path, nworkers=10):\n",
    "    '''Returns each data sample as a tuple (x, y), x = sequence of strings (i.e., syscalls), y = malware program class'''\n",
    "    raw_data_samples = []\n",
    "    files = os.listdir(data_path)\n",
    "    for idx, file in enumerate(files):\n",
    "        loading(idx, len(files))\n",
    "        lines, label = load_content(data_path + \"/\" + file)\n",
    "        raw_data_samples.append((lines, label))\n",
    "    return raw_data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7BSUdq_-sMiw",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading training data ... \n",
      "100.0%\r"
     ]
    }
   ],
   "source": [
    "print('=> Loading training data ... ')\n",
    "train_raw_samples = load_data('./data/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GG6tcSNJsMiw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading testing data ... \n",
      "=> # Train samples =  0\n",
      "=> # Val samples =  0\n",
      "=> # Test  samples =  10000\n"
     ]
    }
   ],
   "source": [
    "project_mode = 'test'    # trainval, traintest, debug\n",
    "np.random.seed(123)          # To perform the same split across multiple runs\n",
    "\n",
    "train = list()\n",
    "val = list()\n",
    "test = list()\n",
    "\n",
    "if project_mode == 'trainval':\n",
    "    l = len(train_raw_samples)\n",
    "    n = int(round(l * 0.8))\n",
    "    train = train_raw_samples[0:n]\n",
    "    val = train_raw_samples[n:l]\n",
    "    \n",
    "elif project_mode == 'test':\n",
    "    print('=> Loading testing data ... ')\n",
    "    test = load_data('./data/test')\n",
    "    \n",
    "elif project_mode == 'debug':\n",
    "    l = len(train_raw_samples)\n",
    "    n = int(round(l * 0.1))\n",
    "    train = train_raw_samples[0:n]\n",
    "    val = train_raw_samples[n+1:2*n+1]\n",
    "else:\n",
    "    raise ValueError('Unrecognized mode')\n",
    "    \n",
    "print('=> # Train samples = ', len(train))\n",
    "print('=> # Val samples = ', len(val))\n",
    "print('=> # Test  samples = ', len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjiNiOVHsMiy"
   },
   "source": [
    "## 1.b. Vectorize: Setup\n",
    "\n",
    "Make one pass over the inputs to identify relevant features/tokens.\n",
    "\n",
    "Suggestion:\n",
    "  - identify tokens (e.g., unigrams, bigrams)\n",
    "  - create a token -> index (int) mapping. Note that you might have a >10K unique tokens. So, you will have to choose a suitable \"vocabulary\" size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DAtmzL6PsMiy"
   },
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "def get_key_idx_map(input_sets, vocab_size, n=1):\n",
    "    # counter is a mapping: token -> count\n",
    "    # build vectorizer using vocab_size most common elements\n",
    "    key_to_idx, idx_to_key = dict(), dict()\n",
    "    for idx, input_set in enumerate(input_sets):\n",
    "        loading(idx, len(input_sets))\n",
    "        key_to_idx = aggregate_dicts(key_to_idx, ngrams(input_set, n))\n",
    "    key_to_idx = cutoff_and_rank(key_to_idx, vocab_size)\n",
    "    idx_to_key = invert_dict(key_to_idx)\n",
    "    return key_to_idx, idx_to_key\n",
    "\n",
    "def ngrams(input_set, n):\n",
    "    new_counter = dict()\n",
    "    for idx in range(len(input_set)):\n",
    "        seq = ngram(input_set, idx, n)\n",
    "        if seq in new_counter:\n",
    "            new_counter[seq] += 1\n",
    "        elif len(seq) == n:\n",
    "            new_counter[seq] = 1\n",
    "    return new_counter\n",
    "\n",
    "def ngram(input_set, idx, n):\n",
    "    return tuple(input_set[idx:idx + n])\n",
    "\n",
    "def aggregate_dicts(d1, d2):\n",
    "    idx = 0\n",
    "    new_dict = copy.deepcopy(d1)\n",
    "    for val in d2:\n",
    "        count = d2[val]\n",
    "        if val in new_dict:\n",
    "            new_dict[val] += count\n",
    "        else:\n",
    "            new_dict[val] = count\n",
    "    return new_dict\n",
    "\n",
    "def cutoff_and_rank(counter, vocab_size):\n",
    "    new_counter = dict()\n",
    "    counter = copy.deepcopy(counter)\n",
    "    rank = 1\n",
    "    vocab_size = min(vocab_size, len(counter.items()))\n",
    "    for _ in range(vocab_size):\n",
    "        max_value = max(counter, key=counter.get)\n",
    "        new_counter[max_value] = rank\n",
    "        counter[max_value] = 0\n",
    "        rank += 1\n",
    "    return new_counter\n",
    "\n",
    "def invert_dict(d):\n",
    "    return {value: key for key, value in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 50\n",
    "path = 'application_vocab_{}.pkl'.format(MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "id": "LSMEAcgvsMiy"
   },
   "outputs": [],
   "source": [
    "uni_grams = get_key_idx_map(list(map(lambda s: s[0], train)), MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, 'wb') as file:\n",
    "    d = {'token_to_idx': uni_grams[0],\n",
    "          'idx_to_token': uni_grams[1]}\n",
    "    pickle.dump(d, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUefzVjFsMiy"
   },
   "source": [
    "## 1.c. Vectorize Data\n",
    "\n",
    "Use the (token $\\rightarrow$ index) mapping you created before to vectorize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ffJJ0XOvsMiz"
   },
   "outputs": [],
   "source": [
    "with open(path, 'rb') as file:\n",
    "    d = pickle.load(file)\n",
    "    token_to_idx = d['token_to_idx']\n",
    "    idx_to_token = d['idx_to_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "CD5IqJmXsMiz"
   },
   "outputs": [],
   "source": [
    "def vectorize_raw_samples(raw_samples, nworkers=10):\n",
    "    vectorized_samples = []\n",
    "    for idx, sample in enumerate(raw_samples):\n",
    "        loading(idx, len(raw_samples))\n",
    "        lines, lable = sample\n",
    "        vectorized_samples.append((vectorize(lines), lable))\n",
    "        idx += 1\n",
    "    return vectorized_samples\n",
    "\n",
    "def vectorize(sample):\n",
    "    counter = list()\n",
    "    for idx in range(len(token_to_idx)):\n",
    "        elem = idx_to_token[idx + 1]\n",
    "        count = occurence(sample, elem)\n",
    "        counter.append(count)\n",
    "    return counter\n",
    "\n",
    "def occurence(lst, obj):\n",
    "    count = 0\n",
    "    for idx in range(len(lst)):\n",
    "        seq = ngram(lst, idx, len(obj))\n",
    "        if seq == obj:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "BtS10ASbsMi0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Processing: Train\n",
      "100.0%\r"
     ]
    }
   ],
   "source": [
    "print('=> Processing: Train')\n",
    "train_data = vectorize_raw_samples(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Processing: Val\n",
      "100.0%\r"
     ]
    }
   ],
   "source": [
    "print('=> Processing: Val')\n",
    "val_data = vectorize_raw_samples(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Processing: Test\n",
      "100.0%\r"
     ]
    }
   ],
   "source": [
    "print('=> Processing: Test')\n",
    "test_data = vectorize_raw_samples(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "0XOHr24usMi0"
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y, test_x, test_y = list(), list(), list(), list(), list(), list()\n",
    "    \n",
    "if project_mode == 'trainval' or project_mode == 'debug':\n",
    "    for vector, label in train_data:\n",
    "        train_x.append(vector)\n",
    "        train_y.append(class_to_int(label))\n",
    "    for vector, label in val_data:\n",
    "        val_x.append(vector)\n",
    "        val_y.append(class_to_int(label))\n",
    "else:\n",
    "    for vector, label in test_data:\n",
    "        test_x.append(vector)\n",
    "        test_y.append(class_to_int(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(train_x) == len(train_y))\n",
    "assert(len(val_x) == len(val_y))\n",
    "assert(len(test_x) == len(test_y))\n",
    "\n",
    "if project_mode == 'trainval' or project_mode == 'debug':\n",
    "    trainset = TensorDataset(torch.tensor(train_x).float(), torch.tensor(train_y))\n",
    "    valset = TensorDataset(torch.tensor(val_x).float(), torch.tensor(val_y))\n",
    "    torch.save(trainset, 'trainset.pt')\n",
    "    torch.save(valset, 'valset.pt')\n",
    "else:\n",
    "    testset = TensorDataset(torch.tensor(test_x).float(), torch.tensor(test_y))\n",
    "    torch.save(testset, 'testset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([297,\n",
       "   263,\n",
       "   6714,\n",
       "   6704,\n",
       "   6702,\n",
       "   3351,\n",
       "   3352,\n",
       "   1038,\n",
       "   35,\n",
       "   401,\n",
       "   16,\n",
       "   12,\n",
       "   0,\n",
       "   0,\n",
       "   348,\n",
       "   348,\n",
       "   346,\n",
       "   346,\n",
       "   346,\n",
       "   346,\n",
       "   346,\n",
       "   346,\n",
       "   288,\n",
       "   0,\n",
       "   199,\n",
       "   0,\n",
       "   0,\n",
       "   182,\n",
       "   172,\n",
       "   24,\n",
       "   173,\n",
       "   173,\n",
       "   173,\n",
       "   173,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   51,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   159,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   25,\n",
       "   25,\n",
       "   25,\n",
       "   0,\n",
       "   4],\n",
       "  'Basun'),\n",
       " ([36790,\n",
       "   36785,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1776,\n",
       "   4,\n",
       "   612,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   592,\n",
       "   600,\n",
       "   592,\n",
       "   592,\n",
       "   592,\n",
       "   592,\n",
       "   592,\n",
       "   592,\n",
       "   0,\n",
       "   0,\n",
       "   310,\n",
       "   0,\n",
       "   0,\n",
       "   305,\n",
       "   298,\n",
       "   1,\n",
       "   296,\n",
       "   296,\n",
       "   296,\n",
       "   296,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   18,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   2],\n",
       "  'Basun'),\n",
       " ([3759,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   44,\n",
       "   38,\n",
       "   14,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   31,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   45,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   8],\n",
       "  'Texel'),\n",
       " ([297,\n",
       "   263,\n",
       "   6459,\n",
       "   6449,\n",
       "   6447,\n",
       "   3223,\n",
       "   3224,\n",
       "   1038,\n",
       "   36,\n",
       "   406,\n",
       "   16,\n",
       "   12,\n",
       "   0,\n",
       "   0,\n",
       "   348,\n",
       "   352,\n",
       "   346,\n",
       "   346,\n",
       "   346,\n",
       "   346,\n",
       "   346,\n",
       "   346,\n",
       "   281,\n",
       "   0,\n",
       "   202,\n",
       "   0,\n",
       "   0,\n",
       "   184,\n",
       "   174,\n",
       "   25,\n",
       "   173,\n",
       "   173,\n",
       "   173,\n",
       "   173,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   52,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   159,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   25,\n",
       "   25,\n",
       "   25,\n",
       "   0,\n",
       "   4],\n",
       "  'Basun'),\n",
       " ([37291,\n",
       "   37286,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1782,\n",
       "   4,\n",
       "   614,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   594,\n",
       "   602,\n",
       "   594,\n",
       "   594,\n",
       "   594,\n",
       "   594,\n",
       "   594,\n",
       "   594,\n",
       "   0,\n",
       "   0,\n",
       "   311,\n",
       "   0,\n",
       "   0,\n",
       "   306,\n",
       "   299,\n",
       "   1,\n",
       "   297,\n",
       "   297,\n",
       "   297,\n",
       "   297,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   18,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   2],\n",
       "  'Basun'),\n",
       " ([0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'Texel'),\n",
       " ([0,\n",
       "   6,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   16,\n",
       "   29,\n",
       "   4,\n",
       "   7,\n",
       "   0,\n",
       "   0,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   309,\n",
       "   0,\n",
       "   28,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   15,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   51,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   2],\n",
       "  'Agent'),\n",
       " ([0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   4,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   53,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'Patched'),\n",
       " ([0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'Texel'),\n",
       " ([0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0],\n",
       "  'Patched')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_l0tLsEVsMi0"
   },
   "source": [
    "# 2. Train Model\n",
    "\n",
    "You will now train an ML model on the vectorized datasets you created previously.\n",
    "\n",
    "_Note_: Although we often refer to each input as a 'vector' for simplicity, each of your inputs can also be higher dimensional tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UInY1YzksMi0"
   },
   "source": [
    "## 2.a. Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VnSv0J75sMi0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def highest(tensor):\n",
    "    max_value = torch.max(tensor)\n",
    "    for idx, val in enumerate(tensor):\n",
    "        if val == max_value:\n",
    "            return idx\n",
    "\n",
    "def save_data(eval_data, out_path):\n",
    "    with open(out_path, 'wb') as wf:\n",
    "        pickle.dump(eval_data, out_path)\n",
    "        \n",
    "def load_set():\n",
    "    trainset = torch.load('trainset.pt')\n",
    "    valset = torch.load('valset.pt')\n",
    "    testset = torch.load('testset.pt')\n",
    "    return trainset, valset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset, testset = load_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhbVzVLDsMi1"
   },
   "source": [
    "## 2.b. Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrUKCNRgsMi1"
   },
   "source": [
    "Describe your model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ILB2fBRWsMi1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fully connected neural networks\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Net, self).__init__()\n",
    "        # Layer definitions\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QO1yPx7YsMi1"
   },
   "source": [
    "## 2.c. Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "0k0jLX5ksMi1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparameters are defined here\n",
    "in_dims = trainset[0][0].shape[0]\n",
    "out_dims = 11\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 8\n",
    "lr = 0.0002\n",
    "\n",
    "malwareClassifier = Net(in_dims, out_dims)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtpgVAO1sMi2"
   },
   "source": [
    "## 2.d. Train your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = []\n",
    "def train_model(model, \n",
    "                train_loader,\n",
    "                valset,\n",
    "                num_epochs,\n",
    "                learning_rate,\n",
    "                loss_function):\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.5)\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(f'---------- Started Epoch {epoch} ----------')\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for train_data in train_loader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            inputs, targets = train_data\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        # every 10 epochs, print accuracy once\n",
    "        if epoch % 10 == 9:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                pred = model(valset[:][0])\n",
    "                accuracy = (torch.argmax(pred, -1) == valset[:][1]).float().mean()\n",
    "                print(f\"Accuracy: {accuracy}\")\n",
    "        loss_value = running_loss/len(train_loader)\n",
    "        loss_values.append(loss_value)\n",
    "        # each epoch, print loss\n",
    "        print(\"Loss: {:.4f}\".format(loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "3afVBWVKsMi2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Started Epoch 0 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 1/50 [00:00<00:33,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8916\n",
      "---------- Started Epoch 1 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 2/50 [00:01<00:32,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8892\n",
      "---------- Started Epoch 2 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 3/50 [00:02<00:31,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8821\n",
      "---------- Started Epoch 3 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 4/50 [00:02<00:30,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8811\n",
      "---------- Started Epoch 4 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 5/50 [00:03<00:30,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8747\n",
      "---------- Started Epoch 5 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 6/50 [00:04<00:29,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8669\n",
      "---------- Started Epoch 6 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 7/50 [00:04<00:28,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8654\n",
      "---------- Started Epoch 7 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█▌        | 8/50 [00:05<00:27,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8600\n",
      "---------- Started Epoch 8 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 9/50 [00:06<00:27,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8673\n",
      "---------- Started Epoch 9 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 10/50 [00:06<00:26,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7174707651138306\n",
      "Loss: 0.8515\n",
      "---------- Started Epoch 10 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 11/50 [00:07<00:26,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8540\n",
      "---------- Started Epoch 11 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 12/50 [00:08<00:25,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8427\n",
      "---------- Started Epoch 12 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 13/50 [00:08<00:24,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8428\n",
      "---------- Started Epoch 13 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 14/50 [00:09<00:24,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8337\n",
      "---------- Started Epoch 14 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 15/50 [00:10<00:23,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8324\n",
      "---------- Started Epoch 15 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███▏      | 16/50 [00:10<00:23,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8400\n",
      "---------- Started Epoch 16 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███▍      | 17/50 [00:11<00:22,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8331\n",
      "---------- Started Epoch 17 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 18/50 [00:12<00:22,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8270\n",
      "---------- Started Epoch 18 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 19/50 [00:12<00:21,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8294\n",
      "---------- Started Epoch 19 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 20/50 [00:13<00:20,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7236841917037964\n",
      "Loss: 0.8251\n",
      "---------- Started Epoch 20 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 21/50 [00:14<00:19,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8167\n",
      "---------- Started Epoch 21 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 22/50 [00:14<00:18,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8096\n",
      "---------- Started Epoch 22 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▌     | 23/50 [00:15<00:18,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8118\n",
      "---------- Started Epoch 23 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 24/50 [00:16<00:17,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8096\n",
      "---------- Started Epoch 24 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 25/50 [00:16<00:16,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8346\n",
      "---------- Started Epoch 25 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████▏    | 26/50 [00:17<00:16,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8233\n",
      "---------- Started Epoch 26 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 27/50 [00:18<00:15,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8079\n",
      "---------- Started Epoch 27 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 28/50 [00:18<00:14,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8051\n",
      "---------- Started Epoch 28 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 29/50 [00:19<00:14,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7992\n",
      "---------- Started Epoch 29 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 30/50 [00:20<00:13,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7244151830673218\n",
      "Loss: 0.7931\n",
      "---------- Started Epoch 30 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▏   | 31/50 [00:20<00:12,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7902\n",
      "---------- Started Epoch 31 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████▍   | 32/50 [00:21<00:11,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7864\n",
      "---------- Started Epoch 32 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 33/50 [00:22<00:11,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7810\n",
      "---------- Started Epoch 33 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 34/50 [00:22<00:10,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7794\n",
      "---------- Started Epoch 34 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 35/50 [00:23<00:09,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7789\n",
      "---------- Started Epoch 35 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████▏  | 36/50 [00:24<00:09,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7762\n",
      "---------- Started Epoch 36 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 37/50 [00:24<00:08,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7835\n",
      "---------- Started Epoch 37 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 38/50 [00:25<00:07,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7913\n",
      "---------- Started Epoch 38 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 39/50 [00:26<00:07,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7739\n",
      "---------- Started Epoch 39 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 40/50 [00:26<00:06,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7266082167625427\n",
      "Loss: 0.7710\n",
      "---------- Started Epoch 40 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|████████▏ | 41/50 [00:27<00:05,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7722\n",
      "---------- Started Epoch 41 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 42/50 [00:28<00:05,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7707\n",
      "---------- Started Epoch 42 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 43/50 [00:28<00:04,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7655\n",
      "---------- Started Epoch 43 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 44/50 [00:29<00:03,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7615\n",
      "---------- Started Epoch 44 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 45/50 [00:30<00:03,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7611\n",
      "---------- Started Epoch 45 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 46/50 [00:30<00:02,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7618\n",
      "---------- Started Epoch 46 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 47/50 [00:31<00:01,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7572\n",
      "---------- Started Epoch 47 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▌| 48/50 [00:32<00:01,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7575\n",
      "---------- Started Epoch 48 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 49/50 [00:32<00:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7519\n",
      "---------- Started Epoch 49 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:33<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7339181303977966\n",
      "Loss: 0.7505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Loaders\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_model(model=malwareClassifier, train_loader=trainloader, valset=valset, num_epochs=n_epochs,\n",
    "            loss_function=loss_func, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5w-7O5jsMi2"
   },
   "source": [
    "## 2.e. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = len(testset)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testset:\n",
    "            outputs = model(inputs)\n",
    "            predicted = highest(outputs.data)\n",
    "            correct += (predicted == labels).sum().item() \n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(model):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = len(valset)\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in valset:\n",
    "#             outputs = model(inputs)\n",
    "#             predicted = highest(outputs.data)\n",
    "#             correct += (predicted == labels).sum().item() \n",
    "#     accuracy = correct / total\n",
    "#     print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.743\n"
     ]
    }
   ],
   "source": [
    "# here I use valset to test since testset isn't well defined\n",
    "test(malwareClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.48      0.54       690\n",
      "           2       0.00      0.00      0.00        94\n",
      "           3       0.73      1.00      0.84       415\n",
      "           4       0.65      0.89      0.75       654\n",
      "           5       0.89      0.98      0.94       103\n",
      "           6       0.91      0.76      0.83       251\n",
      "           7       0.80      0.66      0.72       155\n",
      "           8       0.99      0.99      0.99       167\n",
      "           9       0.70      0.08      0.14        90\n",
      "          10       0.98      0.99      0.99       117\n",
      "\n",
      "    accuracy                           0.73      2736\n",
      "   macro avg       0.73      0.68      0.67      2736\n",
      "weighted avg       0.71      0.73      0.70      2736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.argmax(malwareClassifier(valset[:][0]), -1)\n",
    "# here use classification report to idenfity which malwares are difficult to detect.\n",
    "print(classification_report(valset[:][1], predictions))\n",
    "# the 1 class is missing from the report below ---- in the processed data, I can't find the class 1 as well.\n",
    "# the predictions below should be conducted on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=5, random_state=222).fit(trainset[:][0], trainset[:][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.45      0.46       690\n",
      "           2       0.00      0.00      0.00        94\n",
      "           3       0.75      0.96      0.85       415\n",
      "           4       0.58      0.88      0.70       654\n",
      "           5       0.94      0.66      0.78       103\n",
      "           6       0.86      0.73      0.79       251\n",
      "           7       0.00      0.00      0.00       155\n",
      "           8       1.00      0.99      1.00       167\n",
      "           9       0.00      0.00      0.00        90\n",
      "          10       0.98      0.99      0.99       117\n",
      "\n",
      "    accuracy                           0.66      2736\n",
      "   macro avg       0.56      0.57      0.56      2736\n",
      "weighted avg       0.59      0.66      0.62      2736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(valset[:][1], rf.predict(valset[:][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.11      0.17       690\n",
      "           2       0.00      0.00      0.00        94\n",
      "           3       0.73      1.00      0.84       415\n",
      "           4       0.38      0.98      0.54       654\n",
      "           5       0.00      0.00      0.00       103\n",
      "           6       0.71      0.04      0.08       251\n",
      "           7       0.00      0.00      0.00       155\n",
      "           8       1.00      0.99      1.00       167\n",
      "           9       0.00      0.00      0.00        90\n",
      "          10       0.99      0.99      0.99       117\n",
      "\n",
      "    accuracy                           0.52      2736\n",
      "   macro avg       0.43      0.41      0.36      2736\n",
      "weighted avg       0.48      0.52      0.41      2736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = make_pipeline(StandardScaler(), SVC(gamma='auto')).fit(trainset[:][0], trainset[:][1])\n",
    "print(classification_report(valset[:][1], svm.predict(valset[:][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:02:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.68      0.70       690\n",
      "           2       0.40      0.02      0.04        94\n",
      "           3       0.77      0.91      0.83       415\n",
      "           4       0.74      0.89      0.81       654\n",
      "           5       0.99      1.00      1.00       103\n",
      "           6       0.92      0.83      0.87       251\n",
      "           7       0.93      0.76      0.84       155\n",
      "           8       1.00      0.99      1.00       167\n",
      "           9       0.80      0.44      0.57        90\n",
      "          10       0.98      1.00      0.99       117\n",
      "\n",
      "    accuracy                           0.80      2736\n",
      "   macro avg       0.82      0.75      0.76      2736\n",
      "weighted avg       0.79      0.80      0.78      2736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgboost = XGBClassifier().fit(trainset[:][0].numpy(), trainset[:][1].numpy())\n",
    "print(classification_report(valset[:][1], xgboost.predict(valset[:][0].numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_GE1BQjsMi3"
   },
   "source": [
    "## 2.f. Save Model + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "4Xvo--GIsMi3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the model \n",
    "torch.save(malwareClassifier.state_dict(), \"./malwareClassifier_parameters.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-UQbfCrsMi3"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLD6dur1sMi3"
   },
   "source": [
    "# 3. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QRKxLsPsMi3"
   },
   "source": [
    "## 3.a. Summary: Main Results\n",
    "\n",
    "Summarize your approach and results here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QRKxLsPsMi3"
   },
   "source": [
    "* First we loaded the data up and searched for the most present syscalls. \n",
    "* We stored the 50 most seen ones in a dictionary. These will be used to vectorize files.\n",
    "* Given a file we vectorize it by searching for the number of appearances of said 50 most syscall patterns in the file and assign a vector-list with these values of the appearances. \n",
    "* Now we can turn a MIST file into a classification vector.\n",
    "* On this we trained our fully connected neural network (MLP).\n",
    "* This achieved about 73% accuracy. This result heavily depended on given hyperparameters like batch size though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNKqJAoasMi4"
   },
   "source": [
    "## 3.b. Discussion\n",
    "\n",
    "Enter your final summary here.\n",
    "\n",
    "For instance, you can address:\n",
    "- What was the performance you obtained with the simplest approach?\n",
    "- Which vectorized input representations helped more than the others?\n",
    "- Which malwares are difficult to detect and why?\n",
    "- Which approach do you recommend to perform malware classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNKqJAoasMi4"
   },
   "source": [
    "* The performance for training the model was very decent. The most difficult performance heavy task was definitely vectorizing data because searching for unigrams was not particularely optimized in our code. But I thing our simple unigram approach was helping a bit because it could severely improve runtime compared to bigrams (which we also tested).\n",
    "* In general the most simple approach seemed to yield the best runtime and decent results with a final accuracy.\n",
    "* Generally speaking Basun seems to be difficult to detect as it is not appearing in our classification results. We did not found a particular reason for that.\n",
    "* For a simple and not perfect approach we would recommend for vectorizing MIST output files by simple classifications as uni- or bigrams. For more dedicated approaches one could look into specific patterns of syscalls which might co-occure for better results. Note that vectorization then potentially increases in runtime by a quite significat amount though. For classification a simple but rather effective strategy is using a fully connected neural network as we demonstrated. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Task_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
