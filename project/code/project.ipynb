{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "285fbaa5",
   "metadata": {},
   "source": [
    "## Attacking Apples neural hash\n",
    "\n",
    "This notebook will describe a whitebox attack on the underlying neural network of apples neural hash."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the model:\n",
    "Source and Credits: https://github.com/AsuharietYgvar/AppleNeuralHash2ONNX"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "from PIL import Image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "MODEL_PATH = \"./model.onnx\"\n",
    "OUTPUT_MATRIX_PATH = \"./neuralhash_128x96_seed1.dat\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def neural_hash(image):\n",
    "    # Load ONNX model\n",
    "    session = onnxruntime.InferenceSession(MODEL_PATH)\n",
    "\n",
    "    # Load output hash matrix\n",
    "    seed1 = open(OUTPUT_MATRIX_PATH, 'rb').read()[128:]\n",
    "    seed1 = np.frombuffer(seed1, dtype=np.float32)\n",
    "    seed1 = seed1.reshape([96, 128])\n",
    "\n",
    "    # Preprocess image\n",
    "    image = Image.open(image).convert('RGB')\n",
    "    image = image.resize([360, 360])\n",
    "    arr = np.array(image).astype(np.float32) / 255.0\n",
    "    arr = arr * 2.0 - 1.0\n",
    "    arr = arr.transpose(2, 0, 1).reshape([1, 3, 360, 360])\n",
    "\n",
    "    # Run model\n",
    "    inputs = {session.get_inputs()[0].name: arr}\n",
    "    outs = session.run(None, inputs)\n",
    "\n",
    "    # Convert model output to hex hash\n",
    "    hash_output = seed1.dot(outs[0].flatten())\n",
    "    hash_bits = ''.join(['1' if it >= 0 else '0' for it in hash_output])\n",
    "    hash_hex = '{:0{}x}'.format(int(hash_bits, 2), len(hash_bits) // 4)\n",
    "\n",
    "    return hash_hex"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'55d07b72427978ac7a8fd1d9'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_hash(\"../shrek.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Producing hash collisions\n",
    "Source and Credits: https://github.com/anishathalye/neural-hash-collider"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# util.py\n",
    "\n",
    "import numpy as np\n",
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def load_model(path):\n",
    "    onnx_model = onnx.load(path)\n",
    "    model = prepare(onnx_model, training_mode=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_seed(path):\n",
    "    seed = open(path, 'rb').read()[128:]\n",
    "    seed = np.frombuffer(seed, dtype=np.float32)\n",
    "    seed = seed.reshape([96, 128])\n",
    "    return seed\n",
    "\n",
    "\n",
    "def load_image(path):\n",
    "    im = Image.open(path).convert('RGB')\n",
    "    im = im.resize([360, 360])\n",
    "    arr = np.array(im).astype(np.float32) / 255.0\n",
    "    arr = arr * 2.0 - 1.0\n",
    "    arr = arr.transpose(2, 0, 1).reshape([1, 3, 360, 360])\n",
    "    return arr\n",
    "\n",
    "\n",
    "def save_image(arr, path):\n",
    "    arr = arr.reshape([3, 360, 360]).transpose(1, 2, 0)\n",
    "    arr = (arr + 1.0) * (255.0 / 2.0)\n",
    "    arr = arr.astype(np.uint8)\n",
    "    im = Image.fromarray(arr)\n",
    "    im.save(path)\n",
    "\n",
    "\n",
    "def hash_from_hex(hex_repr):\n",
    "    n = int(hex_repr, 16)\n",
    "    h = np.zeros(96)\n",
    "    for i in range(96):\n",
    "        h[i] = (n >> (95 - i)) & 1\n",
    "    return h\n",
    "\n",
    "\n",
    "def hash_to_hex(h):\n",
    "    bits = ''.join(['1' if i >= 0.5 else '0' for i in h])\n",
    "    return '{:0{}x}'.format(int(bits, 2), len(bits) // 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# collide.py\n",
    "\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import os\n",
    "\n",
    "\n",
    "def collide(image1, image2, o_iterations=1000):\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "    model = load_model(MODEL_PATH)\n",
    "    image = model.tensor_dict['image']\n",
    "    logits = model.tensor_dict['leaf/logits']\n",
    "    seed = load_seed(OUTPUT_MATRIX_PATH)\n",
    "\n",
    "    original = load_image(image1)\n",
    "    h = hash_from_hex(neural_hash(image2))\n",
    "\n",
    "    with model.graph.as_default():\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "            proj = tf.reshape(tf.linalg.matmul(seed, tf.reshape(logits, (128, 1))), (96,))\n",
    "            # proj is in R^96; it's interpreted as a 96-bit hash by mapping\n",
    "            # entries < 0 to the bit '0', and entries >= 0 to the bit '1'\n",
    "            normalized, _ = tf.linalg.normalize(proj)\n",
    "            hash_output = tf.sigmoid(normalized * 10.0)\n",
    "            # now, hash_output has entries in (0, 1); it's interpreted by\n",
    "            # mapping entries < 0.5 to the bit '0' and entries >= 0.5 to the\n",
    "            # bit '1'\n",
    "\n",
    "            # we clip hash_output to (clip_range, 1-clip_range); this seems to\n",
    "            # improve the search (we don't \"waste\" perturbation tweaking\n",
    "            # \"strong\" bits); the sigmoid already does this to some degree, but\n",
    "            # this seems to help\n",
    "            hash_output = tf.clip_by_value(hash_output, 0.1, 0.9) - 0.5\n",
    "            hash_output = hash_output * (0.5 / 0.4)\n",
    "            hash_output = hash_output + 0.5\n",
    "\n",
    "            # hash loss: how far away we are from the target hash\n",
    "            hash_loss = tf.math.reduce_sum(tf.math.squared_difference(hash_output, h))\n",
    "\n",
    "            perturbation = image - original\n",
    "            # image loss: how big / noticeable is the perturbation?\n",
    "            img_loss = 2e-3 * tf.nn.l2_loss(perturbation) + 1e-4 * tf.image.total_variation(perturbation)[0]\n",
    "\n",
    "            # combined loss: try to minimize both at once\n",
    "            combined_loss = 0.8 * hash_loss + 0.2 * img_loss\n",
    "\n",
    "            # gradients of all the losses\n",
    "            g_hash_loss, = tf.gradients(hash_loss, image)\n",
    "            g_img_loss, = tf.gradients(img_loss, image)\n",
    "            g_combined_loss, = tf.gradients(combined_loss, image)\n",
    "\n",
    "            # perform attack\n",
    "\n",
    "            x = original\n",
    "            best = (float('inf'), 0)  # (distance, image quality loss)\n",
    "            dist = float('inf')\n",
    "\n",
    "            for i in range(o_iterations):\n",
    "                # we do an alternating projections style attack here; if we\n",
    "                # haven't found a colliding image yet, only optimize for that;\n",
    "                # if we have a colliding image, then minimize the size of the\n",
    "                # perturbation; if we're close, then do both at once\n",
    "                if dist == 0:\n",
    "                    loss_name, loss, g = 'image', img_loss, g_img_loss\n",
    "                elif best[0] == 0 and dist <= 2:\n",
    "                    loss_name, loss, g = 'combined', combined_loss, g_combined_loss\n",
    "                else:\n",
    "                    loss_name, loss, g = 'hash', hash_loss, g_hash_loss\n",
    "\n",
    "                # compute loss values and gradient\n",
    "                xq = quantize(x)  # take derivatives wrt the quantized version of the image\n",
    "                hash_output_v, img_loss_v, loss_v, g_v = sess.run([hash_output, img_loss, loss, g], feed_dict={image: xq})\n",
    "                dist = np.sum((hash_output_v >= 0.5) != (h >= 0.5))\n",
    "\n",
    "                # if it's better than any image found so far, save it\n",
    "                score = (dist, img_loss_v)\n",
    "                if score < best:\n",
    "                    save_image(x, os.path.join('.', 'out_iter={:05d}_dist={:02d}_q={:.3f}.png'.format(i+1, dist, img_loss_v)))\n",
    "                    best = score\n",
    "\n",
    "\n",
    "                # gradient descent step\n",
    "                g_v_norm = g_v / np.linalg.norm(g_v)\n",
    "                x = x - 2.0 * g_v_norm\n",
    "                x = x.clip(-1, 1)\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    print('iteration: {}/{}, best: ({}, {:.3f}), hash: {}, distance: {}, loss: {:.3f} ({})'.format(\n",
    "                        i+1,\n",
    "                        o_iterations,\n",
    "                        best[0],\n",
    "                        best[1],\n",
    "                        hash_to_hex(hash_output_v),\n",
    "                        dist,\n",
    "                        loss_v,\n",
    "                        loss_name\n",
    "                    ))\n",
    "\n",
    "def quantize(x):\n",
    "    x = (x + 1.0) * (255.0 / 2.0)\n",
    "    x = x.astype(np.uint8).astype(np.float32)\n",
    "    x = x / (255.0 / 2.0) - 1.0\n",
    "    return x\n",
    "\n",
    "\n",
    "def blur_perturbation(original, x, sigma):\n",
    "    perturbation = x - original\n",
    "    perturbation = gaussian_filter_by_channel(perturbation, sigma=sigma)\n",
    "    return original + perturbation\n",
    "\n",
    "\n",
    "def gaussian_filter_by_channel(x, sigma):\n",
    "    return np.stack([gaussian_filter(x[0, ch, :, :], sigma) for ch in range(x.shape[1])])[np.newaxis]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 100/1000, best: (1, 6.297), hash: 1ec173f89d10be5300ac0206, distance: 1, loss: 6.786 (hash)\n",
      "iteration: 200/1000, best: (1, 6.297), hash: 1ec173f89d10be5300ac0206, distance: 1, loss: 6.398 (hash)\n",
      "iteration: 300/1000, best: (1, 6.297), hash: 1ec173f89d10be5300ac0206, distance: 1, loss: 6.329 (hash)\n",
      "iteration: 400/1000, best: (0, 14.153), hash: 1ec173f89d10be5300ac0206, distance: 1, loss: 6.511 (combined)\n",
      "iteration: 500/1000, best: (0, 14.153), hash: 1ec173f89d10be5300ac0206, distance: 1, loss: 6.370 (combined)\n",
      "iteration: 600/1000, best: (0, 14.153), hash: 1ec173f89d10be5300ac0206, distance: 1, loss: 6.340 (combined)\n",
      "iteration: 700/1000, best: (0, 14.153), hash: 1ec173f89d10be5300ac0206, distance: 1, loss: 6.317 (combined)\n",
      "iteration: 800/1000, best: (0, 14.153), hash: 1ec173f89d10be5300ac0206, distance: 1, loss: 6.290 (combined)\n",
      "iteration: 900/1000, best: (0, 14.153), hash: 1ec173f89d10be5300ac0206, distance: 1, loss: 6.280 (combined)\n",
      "iteration: 1000/1000, best: (0, 14.153), hash: 1ec173f89d10be5300ac0206, distance: 1, loss: 6.282 (combined)\n"
     ]
    }
   ],
   "source": [
    "collide(\"../doge.jpeg\", \"../titanic.jpeg\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1:\n",
    "\n",
    "Investigating how much **information leak** through neural-hashing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# imports\n",
    "from openimages.download import download_images\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20  16:42:34 INFO Downloading 500 train images for class 'person'\n",
      "100%|██████████| 500/500 [00:56<00:00,  8.78it/s]\n",
      "2022-01-20  16:43:31 INFO Downloading 500 train images for class 'wheel'\n",
      "100%|██████████| 500/500 [01:00<00:00,  8.24it/s]\n",
      "2022-01-20  16:44:32 INFO Downloading 500 train images for class 'human hair'\n",
      "100%|██████████| 500/500 [00:57<00:00,  8.67it/s]\n",
      "2022-01-20  16:45:30 INFO Downloading 500 train images for class 'flower'\n",
      "100%|██████████| 500/500 [01:00<00:00,  8.32it/s]\n",
      "2022-01-20  16:46:30 INFO Downloading 500 train images for class 'window'\n",
      "100%|██████████| 500/500 [00:58<00:00,  8.51it/s]\n",
      "2022-01-20  16:47:29 INFO Downloading 500 train images for class 'house'\n",
      "100%|██████████| 500/500 [00:57<00:00,  8.69it/s]\n",
      "2022-01-20  16:48:27 INFO Downloading 500 train images for class 'table'\n",
      "100%|██████████| 500/500 [00:56<00:00,  8.80it/s]\n",
      "2022-01-20  16:49:24 INFO Downloading 500 train images for class 'glasses'\n",
      "100%|██████████| 500/500 [01:02<00:00,  8.01it/s]\n",
      "2022-01-20  16:50:26 INFO Downloading 500 train images for class 'tree'\n",
      "100%|██████████| 500/500 [01:03<00:00,  7.87it/s]\n",
      "2022-01-20  16:51:30 INFO Downloading 500 train images for class 'man'\n",
      "100%|██████████| 500/500 [01:01<00:00,  8.14it/s]\n",
      "2022-01-20  16:52:32 INFO Downloading 500 train images for class 'plant'\n",
      "100%|██████████| 500/500 [00:58<00:00,  8.62it/s]\n",
      "2022-01-20  16:53:30 INFO Downloading 500 train images for class 'footwear'\n",
      "100%|██████████| 500/500 [00:57<00:00,  8.64it/s]\n",
      "2022-01-20  16:54:28 INFO Downloading 500 train images for class 'woman'\n",
      "100%|██████████| 500/500 [01:14<00:00,  6.70it/s]\n",
      "2022-01-20  16:55:43 INFO Downloading 500 train images for class 'girl'\n",
      "100%|██████████| 500/500 [01:13<00:00,  6.84it/s]\n",
      "2022-01-20  16:56:56 INFO Downloading 500 train images for class 'boy'\n",
      "100%|██████████| 500/500 [00:56<00:00,  8.91it/s]\n",
      "2022-01-20  16:57:53 INFO Downloading 500 train images for class 'clothing'\n",
      "100%|██████████| 500/500 [00:57<00:00,  8.62it/s]\n",
      "2022-01-20  16:58:51 INFO Downloading 500 train images for class 'building'\n",
      "100%|██████████| 500/500 [00:58<00:00,  8.58it/s]\n",
      "2022-01-20  16:59:50 INFO Downloading 500 train images for class 'car'\n",
      "100%|██████████| 500/500 [01:02<00:00,  8.06it/s]\n",
      "2022-01-20  17:00:52 INFO Downloading 500 train images for class 'dress'\n",
      "100%|██████████| 500/500 [00:57<00:00,  8.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'person': '../datasets/person/images',\n 'wheel': '../datasets/wheel/images',\n 'human hair': '../datasets/human hair/images',\n 'flower': '../datasets/flower/images',\n 'window': '../datasets/window/images',\n 'house': '../datasets/house/images',\n 'table': '../datasets/table/images',\n 'glasses': '../datasets/glasses/images',\n 'tree': '../datasets/tree/images',\n 'man': '../datasets/man/images',\n 'plant': '../datasets/plant/images',\n 'footwear': '../datasets/footwear/images',\n 'woman': '../datasets/woman/images',\n 'girl': '../datasets/girl/images',\n 'boy': '../datasets/boy/images',\n 'clothing': '../datasets/clothing/images',\n 'building': '../datasets/building/images',\n 'car': '../datasets/car/images',\n 'dress': '../datasets/dress/images'}"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset(s)\n",
    "\n",
    "download_images(\"../datasets\", [\"Person\", \"Wheel\", \"Human hair\",\n",
    "                                \"Flower\", \"Window\", \"House\",\n",
    "                                \"Table\", \"Glasses\", \"Tree\",\n",
    "                                \"Man\", \"Plant\", \"Footwear\",\n",
    "                                \"Woman\", \"Girl\", \"Boy\", \"Clothing\",\n",
    "                                \"Building\", \"Car\", \"Dress\"],\n",
    "                \"../exclusion\", limit=500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifiers: ['man', 'tree', 'car', 'footwear', 'boy', 'plant', 'person', 'flower', 'clothing', 'dress', 'wheel', 'glasses', 'table', 'window', 'girl', 'house', 'human hair', 'woman', 'building']\n",
      "hashing 500 images [man]\n",
      "hashing 500 images [tree]\n",
      "hashing 500 images [car]\n",
      "hashing 500 images [footwear]\n",
      "hashing 500 images [boy]\n",
      "hashing 500 images [plant]\n",
      "hashing 500 images [person]\n",
      "hashing 500 images [flower]\n",
      "hashing 500 images [clothing]\n",
      "hashing 500 images [dress]\n",
      "hashing 500 images [wheel]\n",
      "hashing 500 images [glasses]\n",
      "hashing 500 images [table]\n",
      "hashing 500 images [window]\n",
      "hashing 500 images [girl]\n",
      "hashing 500 images [house]\n",
      "hashing 500 images [human hair]\n",
      "hashing 500 images [woman]\n",
      "hashing 500 images [building]\n"
     ]
    }
   ],
   "source": [
    "# Hash dataset\n",
    "\n",
    "base = \"../datasets/\"\n",
    "ext = \"/images/\"\n",
    "labels = os.listdir(base)\n",
    "\n",
    "with open(\"hashes.txt\", \"w\") as fd:\n",
    "    print(f\"classifiers: {labels}\")\n",
    "    for label in labels:\n",
    "        files = os.listdir(base + label + ext)\n",
    "        print(f\"hashing {len(files)} images [{label}]\")\n",
    "        for file in files:\n",
    "            fd.write(neural_hash(base + label + ext + file) + \", \" + label + \"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define new model to classify hashes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2:\n",
    "\n",
    "Detection Evasion using simple image altering"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# Define image manipulation functions\n",
    "\n",
    "def function1(image):\n",
    "    return image\n",
    "\n",
    "def function2(image):\n",
    "    return image\n",
    "\n",
    "def function3(image):\n",
    "    return image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Transform datasets using manipulation functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "52"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform neural hash on transformed and non-transformed\n",
    "# dataset, then evaluate amount of changed bits in hash\n",
    "\n",
    "# calculate the bitwise difference between two neural hashes\n",
    "def difference(hash1: str, hash2: str):\n",
    "    one = format(int(hash1, 16), \"#098b\")[2:]\n",
    "    two = format(int(hash2, 16), \"#098b\")[2:]\n",
    "    return sum([one[i] != two[i] for i in range(96)])\n",
    "\n",
    "# evaluate difference in hashes w & w/o filter on\n",
    "def filter_difference(image, function, *kwargs):\n",
    "    return difference(\n",
    "        neural_hash(image),\n",
    "        neural_hash(function(image, *kwargs))\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}